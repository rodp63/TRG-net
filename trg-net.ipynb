{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c0c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import Tuple, List\n",
    "\n",
    "from torch import nn\n",
    "from torchvision.models import mobilenet_v2\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa2a2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = mobilenet_v2(pretrained=True).features\n",
    "backbone.out_channels = 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be124d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec008e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29c3e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03667d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d9204e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a13435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRGnet(nn.Module):\n",
    "    \"\"\"\n",
    "    Main class for Generalized R-CNN.\n",
    "\n",
    "    Args:\n",
    "        backbone (nn.Module):\n",
    "        rpn (nn.Module):\n",
    "        roi_heads (nn.Module): takes the features + the proposals from the RPN and computes\n",
    "            detections / masks from it.\n",
    "        transform (nn.Module): performs the data transformation from the inputs to feed into\n",
    "            the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone: nn.Module, rpm: nn.Module, roi_heads: nn.Module, transform: nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        self.backbone = backbone\n",
    "        self.rpm = rpm\n",
    "        self.roi_heads = roi_heads\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list[Tensor]): images to be processed\n",
    "            targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n",
    "\n",
    "        Returns:\n",
    "            result (list[BoxList] or dict[Tensor]): the output from the model.\n",
    "                During training, it returns a dict[Tensor] which contains the losses.\n",
    "                During testing, it returns list[BoxList] contains additional fields\n",
    "                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
    "\n",
    "        \"\"\"\n",
    "        if self.training and targets is None:\n",
    "            raise ValueError(\"In training mode, targets should be passed\")\n",
    "        if self.training:\n",
    "            assert targets is not None\n",
    "            for target in targets:\n",
    "                boxes = target[\"boxes\"]\n",
    "                if isinstance(boxes, torch.Tensor):\n",
    "                    if len(boxes.shape) != 2 or boxes.shape[-1] != 4:\n",
    "                        raise ValueError(f\"Expected target boxes to be a tensor of shape [N, 4], got {boxes.shape}.\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Expected target boxes to be of type Tensor, got {type(boxes)}.\")\n",
    "\n",
    "        original_image_sizes: List[Tuple[int, int]] = []\n",
    "        for img in images:\n",
    "            val = img.shape[-2:]\n",
    "            assert len(val) == 2\n",
    "            original_image_sizes.append((val[0], val[1]))\n",
    "\n",
    "        images, targets = self.transform(images, targets)\n",
    "\n",
    "        # Check for degenerate boxes\n",
    "        # TODO: Move this to a function\n",
    "        if targets is not None:\n",
    "            for target_idx, target in enumerate(targets):\n",
    "                boxes = target[\"boxes\"]\n",
    "                degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n",
    "                if degenerate_boxes.any():\n",
    "                    # print the first degenerate box\n",
    "                    bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n",
    "                    degen_bb: List[float] = boxes[bb_idx].tolist()\n",
    "                    raise ValueError(\n",
    "                        \"All bounding boxes should have positive height and width.\"\n",
    "                        f\" Found invalid box {degen_bb} for target at index {target_idx}.\"\n",
    "                    )\n",
    "\n",
    "        features = self.backbone(images.tensors)\n",
    "        if isinstance(features, torch.Tensor):\n",
    "            features = OrderedDict([(\"0\", features)])\n",
    "        \n",
    "        proposals, proposal_losses = self.rpm(images, features, targets)\n",
    "        detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)\n",
    "        detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)\n",
    "\n",
    "        losses = {}\n",
    "        losses.update(detector_losses)\n",
    "\n",
    "        if self.training:\n",
    "            return losses\n",
    "\n",
    "        return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6344c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "trg = TRGnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ff45e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
